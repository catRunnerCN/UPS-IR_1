{
    "pdf_info": [
        {
            "para_blocks": [
                {
                    "bbox": [
                        209,
                        147,
                        400,
                        164
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                209,
                                147,
                                400,
                                164
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        209,
                                        147,
                                        400,
                                        164
                                    ],
                                    "type": "text",
                                    "content": "Attention Is All You Need"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        131,
                        233,
                        205,
                        243
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                131,
                                233,
                                205,
                                243
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        131,
                                        233,
                                        205,
                                        243
                                    ],
                                    "type": "text",
                                    "content": "Ashish Vaswani*"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        137,
                        245,
                        194,
                        255
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                137,
                                245,
                                194,
                                255
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        137,
                                        245,
                                        194,
                                        255
                                    ],
                                    "type": "text",
                                    "content": "Google Brain"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        115,
                        256,
                        216,
                        266
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                115,
                                256,
                                216,
                                266
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        115,
                                        256,
                                        216,
                                        266
                                    ],
                                    "type": "text",
                                    "content": "avaswani@google.com"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        237,
                        233,
                        305,
                        243
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                237,
                                233,
                                305,
                                243
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        237,
                                        233,
                                        305,
                                        243
                                    ],
                                    "type": "text",
                                    "content": "Noam Shazeer*"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        228,
                        245,
                        309,
                        266
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                228,
                                245,
                                309,
                                266
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        228,
                                        245,
                                        309,
                                        266
                                    ],
                                    "type": "text",
                                    "content": "Google Brain noam@google.com"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        337,
                        233,
                        396,
                        243
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                337,
                                233,
                                396,
                                243
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        337,
                                        233,
                                        396,
                                        243
                                    ],
                                    "type": "text",
                                    "content": "Niki Parmar*"
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        322,
                        245,
                        407,
                        266
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                322,
                                245,
                                407,
                                266
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        322,
                                        245,
                                        407,
                                        266
                                    ],
                                    "type": "text",
                                    "content": "Google Research nikip@google.com"
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        422,
                        233,
                        497,
                        243
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                422,
                                233,
                                497,
                                243
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        422,
                                        233,
                                        497,
                                        243
                                    ],
                                    "type": "text",
                                    "content": "Jakob Uszkoreit*"
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        421,
                        245,
                        495,
                        266
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                421,
                                245,
                                495,
                                266
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        421,
                                        245,
                                        495,
                                        266
                                    ],
                                    "type": "text",
                                    "content": "Google Research usz@google.com"
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        142,
                        283,
                        197,
                        293
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                142,
                                283,
                                197,
                                293
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        142,
                                        283,
                                        197,
                                        293
                                    ],
                                    "type": "text",
                                    "content": "Llion Jones*"
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "bbox": [
                        133,
                        294,
                        203,
                        305
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                133,
                                294,
                                203,
                                305
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        133,
                                        294,
                                        203,
                                        305
                                    ],
                                    "type": "text",
                                    "content": "Google Research"
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        125,
                        306,
                        211,
                        317
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                125,
                                306,
                                211,
                                317
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        125,
                                        306,
                                        211,
                                        317
                                    ],
                                    "type": "text",
                                    "content": "llion@google.com"
                                }
                            ]
                        }
                    ],
                    "index": 13
                },
                {
                    "bbox": [
                        247,
                        283,
                        330,
                        293
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                247,
                                283,
                                330,
                                293
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        247,
                                        283,
                                        330,
                                        293
                                    ],
                                    "type": "text",
                                    "content": "Aidan N. Gomez*†"
                                }
                            ]
                        }
                    ],
                    "index": 14
                },
                {
                    "bbox": [
                        244,
                        294,
                        340,
                        316
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                244,
                                294,
                                340,
                                316
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        244,
                                        294,
                                        340,
                                        316
                                    ],
                                    "type": "text",
                                    "content": "University of Toronto. dan@cs.toronto.edu"
                                }
                            ]
                        }
                    ],
                    "index": 15
                },
                {
                    "bbox": [
                        392,
                        283,
                        460,
                        293
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                392,
                                283,
                                460,
                                293
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        392,
                                        283,
                                        460,
                                        293
                                    ],
                                    "type": "text",
                                    "content": "Lukasz Kaiser*"
                                }
                            ]
                        }
                    ],
                    "index": 16
                },
                {
                    "bbox": [
                        395,
                        294,
                        452,
                        305
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                395,
                                294,
                                452,
                                305
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        395,
                                        294,
                                        452,
                                        305
                                    ],
                                    "type": "text",
                                    "content": "Google Brain"
                                }
                            ]
                        }
                    ],
                    "index": 17
                },
                {
                    "bbox": [
                        363,
                        306,
                        485,
                        317
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                363,
                                306,
                                485,
                                317
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        363,
                                        306,
                                        485,
                                        317
                                    ],
                                    "type": "text",
                                    "content": "lukaszkaiser@google.com"
                                }
                            ]
                        }
                    ],
                    "index": 18
                },
                {
                    "bbox": [
                        266,
                        332,
                        347,
                        343
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                266,
                                332,
                                347,
                                343
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        266,
                                        332,
                                        347,
                                        343
                                    ],
                                    "type": "text",
                                    "content": "Illia Polosukhin* †"
                                }
                            ]
                        }
                    ],
                    "index": 19
                },
                {
                    "bbox": [
                        236,
                        345,
                        374,
                        356
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                236,
                                345,
                                374,
                                356
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        236,
                                        345,
                                        374,
                                        356
                                    ],
                                    "type": "text",
                                    "content": "illia.polosukhin@gmail.com"
                                }
                            ]
                        }
                    ],
                    "index": 20
                },
                {
                    "bbox": [
                        281,
                        384,
                        329,
                        396
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                281,
                                384,
                                329,
                                396
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        281,
                                        384,
                                        329,
                                        396
                                    ],
                                    "type": "text",
                                    "content": "Abstract"
                                }
                            ]
                        }
                    ],
                    "index": 21
                },
                {
                    "bbox": [
                        140,
                        411,
                        470,
                        577
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                140,
                                411,
                                470,
                                577
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        140,
                                        411,
                                        470,
                                        577
                                    ],
                                    "type": "text",
                                    "content": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                                }
                            ]
                        }
                    ],
                    "index": 22
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        122,
                        71,
                        489,
                        113
                    ],
                    "type": "header",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                122,
                                71,
                                489,
                                113
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        122,
                                        71,
                                        489,
                                        113
                                    ],
                                    "type": "text",
                                    "content": "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        597,
                        505,
                        686
                    ],
                    "type": "page_footnote",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                597,
                                505,
                                686
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        597,
                                        505,
                                        686
                                    ],
                                    "type": "text",
                                    "content": "*Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research."
                                }
                            ]
                        }
                    ],
                    "index": 23
                },
                {
                    "bbox": [
                        118,
                        687,
                        266,
                        698
                    ],
                    "type": "page_footnote",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                118,
                                687,
                                266,
                                698
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        118,
                                        687,
                                        266,
                                        698
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\dagger"
                                },
                                {
                                    "bbox": [
                                        118,
                                        687,
                                        266,
                                        698
                                    ],
                                    "type": "text",
                                    "content": " Work performed while at Google Brain."
                                }
                            ]
                        }
                    ],
                    "index": 24
                },
                {
                    "bbox": [
                        118,
                        698,
                        279,
                        709
                    ],
                    "type": "page_footnote",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                118,
                                698,
                                279,
                                709
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        118,
                                        698,
                                        279,
                                        709
                                    ],
                                    "type": "inline_equation",
                                    "content": "{}^{ \\ddagger  }"
                                },
                                {
                                    "bbox": [
                                        118,
                                        698,
                                        279,
                                        709
                                    ],
                                    "type": "text",
                                    "content": " Work performed while at Google Research."
                                }
                            ]
                        }
                    ],
                    "index": 25
                },
                {
                    "bbox": [
                        105,
                        731,
                        459,
                        742
                    ],
                    "type": "footer",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                731,
                                459,
                                742
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        731,
                                        459,
                                        742
                                    ],
                                    "type": "text",
                                    "content": "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA."
                                }
                            ]
                        }
                    ],
                    "index": 27
                },
                {
                    "bbox": [
                        14,
                        212,
                        37,
                        555
                    ],
                    "type": "aside_text",
                    "angle": 270,
                    "lines": [
                        {
                            "bbox": [
                                14,
                                212,
                                37,
                                555
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        14,
                                        212,
                                        37,
                                        555
                                    ],
                                    "type": "text",
                                    "content": "arXiv:1706.03762v7 [cs.CL] 2 Aug 2023"
                                }
                            ]
                        }
                    ],
                    "index": 28
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 0
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        71,
                        192,
                        84
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                106,
                                71,
                                192,
                                84
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        106,
                                        71,
                                        192,
                                        84
                                    ],
                                    "type": "text",
                                    "content": "1 Introduction"
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        97,
                        504,
                        152
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                97,
                                504,
                                152
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        97,
                                        504,
                                        152
                                    ],
                                    "type": "text",
                                    "content": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]."
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        157,
                        506,
                        245
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                157,
                                506,
                                245
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        157,
                                        506,
                                        245
                                    ],
                                    "type": "text",
                                    "content": "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states "
                                },
                                {
                                    "bbox": [
                                        104,
                                        157,
                                        506,
                                        245
                                    ],
                                    "type": "inline_equation",
                                    "content": "h_t"
                                },
                                {
                                    "bbox": [
                                        104,
                                        157,
                                        506,
                                        245
                                    ],
                                    "type": "text",
                                    "content": ", as a function of the previous hidden state "
                                },
                                {
                                    "bbox": [
                                        104,
                                        157,
                                        506,
                                        245
                                    ],
                                    "type": "inline_equation",
                                    "content": "h_{t-1}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        157,
                                        506,
                                        245
                                    ],
                                    "type": "text",
                                    "content": " and the input for position "
                                },
                                {
                                    "bbox": [
                                        104,
                                        157,
                                        506,
                                        245
                                    ],
                                    "type": "inline_equation",
                                    "content": "t"
                                },
                                {
                                    "bbox": [
                                        104,
                                        157,
                                        506,
                                        245
                                    ],
                                    "type": "text",
                                    "content": ". This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        251,
                        506,
                        295
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                251,
                                506,
                                295
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        251,
                                        506,
                                        295
                                    ],
                                    "type": "text",
                                    "content": "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        299,
                        507,
                        344
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                299,
                                507,
                                344
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        299,
                                        507,
                                        344
                                    ],
                                    "type": "text",
                                    "content": "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        105,
                        361,
                        189,
                        374
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                361,
                                189,
                                374
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        361,
                                        189,
                                        374
                                    ],
                                    "type": "text",
                                    "content": "2 Background"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        387,
                        506,
                        485
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                387,
                                506,
                                485
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        387,
                                        506,
                                        485
                                    ],
                                    "type": "text",
                                    "content": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        491,
                        506,
                        536
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                491,
                                506,
                                536
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        491,
                                        506,
                                        536
                                    ],
                                    "type": "text",
                                    "content": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        540,
                        506,
                        574
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                540,
                                506,
                                574
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        540,
                                        506,
                                        574
                                    ],
                                    "type": "text",
                                    "content": "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]."
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        104,
                        578,
                        506,
                        624
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                578,
                                506,
                                624
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        578,
                                        506,
                                        624
                                    ],
                                    "type": "text",
                                    "content": "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]."
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        105,
                        641,
                        227,
                        653
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                641,
                                227,
                                653
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        641,
                                        227,
                                        653
                                    ],
                                    "type": "text",
                                    "content": "3 Model Architecture"
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        104,
                        667,
                        506,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                667,
                                506,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations "
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "(x_{1},\\ldots ,x_{n})"
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": " to a sequence of continuous representations "
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{z} = (z_1,\\dots,z_n)"
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": ". Given "
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{z}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": ", the decoder then generates an output sequence "
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "(y_{1},\\dots,y_{m})"
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": " of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next."
                                }
                            ]
                        }
                    ],
                    "index": 11
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "2"
                                }
                            ]
                        }
                    ],
                    "index": 12
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 1
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        194,
                        70,
                        416,
                        395
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                194,
                                70,
                                416,
                                395
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        194,
                                        70,
                                        416,
                                        395
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                194,
                                                70,
                                                416,
                                                395
                                            ],
                                            "type": "image",
                                            "image_path": "d018247de7540bbbd7d638e7b3a9aa21d04567cb8492ac4ce39dc5526098c0b6.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                207,
                                403,
                                402,
                                415
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        207,
                                        403,
                                        402,
                                        415
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                207,
                                                403,
                                                402,
                                                415
                                            ],
                                            "type": "text",
                                            "content": "Figure 1: The Transformer - model architecture."
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        434,
                        506,
                        468
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                434,
                                506,
                                468
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        434,
                                        506,
                                        468
                                    ],
                                    "type": "text",
                                    "content": "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        105,
                        481,
                        254,
                        492
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                481,
                                254,
                                492
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        481,
                                        254,
                                        492
                                    ],
                                    "type": "text",
                                    "content": "3.1 Encoder and Decoder Stacks"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        501,
                        506,
                        578
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                501,
                                506,
                                578
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "text",
                                    "content": "Encoder: The encoder is composed of a stack of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "inline_equation",
                                    "content": "N = 6"
                                },
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "text",
                                    "content": " identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm "
                                },
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "inline_equation",
                                    "content": "(x + \\mathrm{Sublayer}(x))"
                                },
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "text",
                                    "content": ", where Sublayer "
                                },
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "inline_equation",
                                    "content": "(x)"
                                },
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "text",
                                    "content": " is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension "
                                },
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{\\mathrm{model}} = 512"
                                },
                                {
                                    "bbox": [
                                        104,
                                        501,
                                        506,
                                        578
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        104,
                        590,
                        506,
                        668
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                590,
                                506,
                                668
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        590,
                                        506,
                                        668
                                    ],
                                    "type": "text",
                                    "content": "Decoder: The decoder is also composed of a stack of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        590,
                                        506,
                                        668
                                    ],
                                    "type": "inline_equation",
                                    "content": "N = 6"
                                },
                                {
                                    "bbox": [
                                        104,
                                        590,
                                        506,
                                        668
                                    ],
                                    "type": "text",
                                    "content": " identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position "
                                },
                                {
                                    "bbox": [
                                        104,
                                        590,
                                        506,
                                        668
                                    ],
                                    "type": "inline_equation",
                                    "content": "i"
                                },
                                {
                                    "bbox": [
                                        104,
                                        590,
                                        506,
                                        668
                                    ],
                                    "type": "text",
                                    "content": " can depend only on the known outputs at positions less than "
                                },
                                {
                                    "bbox": [
                                        104,
                                        590,
                                        506,
                                        668
                                    ],
                                    "type": "inline_equation",
                                    "content": "i"
                                },
                                {
                                    "bbox": [
                                        104,
                                        590,
                                        506,
                                        668
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        105,
                        679,
                        171,
                        690
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                679,
                                171,
                                690
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        679,
                                        171,
                                        690
                                    ],
                                    "type": "text",
                                    "content": "3.2 Attention"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        700,
                        506,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                700,
                                506,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        700,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum"
                                }
                            ]
                        }
                    ],
                    "index": 7
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        308,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                308,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        308,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "3"
                                }
                            ]
                        }
                    ],
                    "index": 8
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 2
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        171,
                        91,
                        241,
                        222
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                171,
                                91,
                                241,
                                222
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        171,
                                        91,
                                        241,
                                        222
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                171,
                                                91,
                                                241,
                                                222
                                            ],
                                            "type": "image",
                                            "image_path": "be12fe5cdac40d7bba8036ae5bdc9c7f235860dfab1966e90dcd633db77b3244.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                145,
                                68,
                                268,
                                80
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        145,
                                        68,
                                        268,
                                        80
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                145,
                                                68,
                                                268,
                                                80
                                            ],
                                            "type": "text",
                                            "content": "Scaled Dot-Product Attention"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 1
                },
                {
                    "type": "image",
                    "bbox": [
                        345,
                        82,
                        468,
                        239
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                345,
                                82,
                                468,
                                239
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        345,
                                        82,
                                        468,
                                        239
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                345,
                                                82,
                                                468,
                                                239
                                            ],
                                            "type": "image",
                                            "image_path": "56290d9a2b14958c7799ce44e5317291632444e9a20a500b30ae7a04a020b0a2.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 3,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                104,
                                272,
                                504,
                                297
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        272,
                                        504,
                                        297
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                272,
                                                504,
                                                297
                                            ],
                                            "type": "text",
                                            "content": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel."
                                        }
                                    ]
                                }
                            ],
                            "index": 4,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                361,
                                68,
                                451,
                                80
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        361,
                                        68,
                                        451,
                                        80
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                361,
                                                68,
                                                451,
                                                80
                                            ],
                                            "type": "text",
                                            "content": "Multi-Head Attention"
                                        }
                                    ]
                                }
                            ],
                            "index": 2,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        315,
                        504,
                        339
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                315,
                                504,
                                339
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        315,
                                        504,
                                        339
                                    ],
                                    "type": "text",
                                    "content": "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        105,
                        349,
                        264,
                        361
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                349,
                                264,
                                361
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        349,
                                        264,
                                        361
                                    ],
                                    "type": "text",
                                    "content": "3.2.1 Scaled Dot-Product Attention"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        367,
                        504,
                        411
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                367,
                                504,
                                411
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        367,
                                        504,
                                        411
                                    ],
                                    "type": "text",
                                    "content": "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension "
                                },
                                {
                                    "bbox": [
                                        104,
                                        367,
                                        504,
                                        411
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{k}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        367,
                                        504,
                                        411
                                    ],
                                    "type": "text",
                                    "content": ", and values of dimension "
                                },
                                {
                                    "bbox": [
                                        104,
                                        367,
                                        504,
                                        411
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{v}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        367,
                                        504,
                                        411
                                    ],
                                    "type": "text",
                                    "content": ". We compute the dot products of the query with all keys, divide each by "
                                },
                                {
                                    "bbox": [
                                        104,
                                        367,
                                        504,
                                        411
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\sqrt{d_k}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        367,
                                        504,
                                        411
                                    ],
                                    "type": "text",
                                    "content": ", and apply a softmax function to obtain the weights on the values."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        417,
                        504,
                        451
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                417,
                                504,
                                451
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        417,
                                        504,
                                        451
                                    ],
                                    "type": "text",
                                    "content": "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix "
                                },
                                {
                                    "bbox": [
                                        104,
                                        417,
                                        504,
                                        451
                                    ],
                                    "type": "inline_equation",
                                    "content": "Q"
                                },
                                {
                                    "bbox": [
                                        104,
                                        417,
                                        504,
                                        451
                                    ],
                                    "type": "text",
                                    "content": ". The keys and values are also packed together into matrices "
                                },
                                {
                                    "bbox": [
                                        104,
                                        417,
                                        504,
                                        451
                                    ],
                                    "type": "inline_equation",
                                    "content": "K"
                                },
                                {
                                    "bbox": [
                                        104,
                                        417,
                                        504,
                                        451
                                    ],
                                    "type": "text",
                                    "content": " and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        417,
                                        504,
                                        451
                                    ],
                                    "type": "inline_equation",
                                    "content": "V"
                                },
                                {
                                    "bbox": [
                                        104,
                                        417,
                                        504,
                                        451
                                    ],
                                    "type": "text",
                                    "content": ". We compute the matrix of outputs as:"
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        217,
                        463,
                        505,
                        491
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                217,
                                463,
                                505,
                                491
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        217,
                                        463,
                                        505,
                                        491
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\operatorname {A t t e n t i o n} (Q, K, V) = \\operatorname {s o f t m a x} \\left(\\frac {Q K ^ {T}}{\\sqrt {d _ {k}}}\\right) V \\tag {1}",
                                    "image_path": "522289bb889e18ba9d80ac3acaf9af923b7ef1c0cd5466e6eb6c40308f5245d2.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        104,
                        498,
                        504,
                        567
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                498,
                                504,
                                567
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        498,
                                        504,
                                        567
                                    ],
                                    "type": "text",
                                    "content": "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        498,
                                        504,
                                        567
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\frac{1}{\\sqrt{d_k}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        498,
                                        504,
                                        567
                                    ],
                                    "type": "text",
                                    "content": ". Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code."
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        104,
                        571,
                        504,
                        619
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                571,
                                504,
                                619
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "text",
                                    "content": "While for small values of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{k}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "text",
                                    "content": " the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{k}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "text",
                                    "content": " [3]. We suspect that for large values of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{k}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "text",
                                    "content": ", the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients<sup>4</sup>. To counteract this effect, we scale the dot products by "
                                },
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\frac{1}{\\sqrt{d_k}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        571,
                                        504,
                                        619
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "bbox": [
                        105,
                        628,
                        231,
                        639
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                628,
                                231,
                                639
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        628,
                                        231,
                                        639
                                    ],
                                    "type": "text",
                                    "content": "3.2.2 Multi-Head Attention"
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        104,
                        647,
                        505,
                        693
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                647,
                                505,
                                693
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "text",
                                    "content": "Instead of performing a single attention function with "
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{\\mathrm{model}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "text",
                                    "content": "-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values "
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "inline_equation",
                                    "content": "h"
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "text",
                                    "content": " times with different, learned linear projections to "
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_k"
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "text",
                                    "content": ", "
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_k"
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "text",
                                    "content": " and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_v"
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "text",
                                    "content": " dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding "
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_v"
                                },
                                {
                                    "bbox": [
                                        104,
                                        647,
                                        505,
                                        693
                                    ],
                                    "type": "text",
                                    "content": "-dimensional"
                                }
                            ]
                        }
                    ],
                    "index": 13
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        104,
                        699,
                        505,
                        723
                    ],
                    "type": "page_footnote",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                699,
                                505,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        699,
                                        505,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "4To illustrate why the dot products get large, assume that the components of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        699,
                                        505,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "q"
                                },
                                {
                                    "bbox": [
                                        104,
                                        699,
                                        505,
                                        723
                                    ],
                                    "type": "text",
                                    "content": " and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        699,
                                        505,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "k"
                                },
                                {
                                    "bbox": [
                                        104,
                                        699,
                                        505,
                                        723
                                    ],
                                    "type": "text",
                                    "content": " are independent random variables with mean 0 and variance 1. Then their dot product, "
                                },
                                {
                                    "bbox": [
                                        104,
                                        699,
                                        505,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "q\\cdot k = \\sum_{i = 1}^{d_k}q_ik_i"
                                },
                                {
                                    "bbox": [
                                        104,
                                        699,
                                        505,
                                        723
                                    ],
                                    "type": "text",
                                    "content": " has mean 0 and variance "
                                },
                                {
                                    "bbox": [
                                        104,
                                        699,
                                        505,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{k}"
                                }
                            ]
                        }
                    ],
                    "index": 14
                },
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "4"
                                }
                            ]
                        }
                    ],
                    "index": 15
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 3
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        104,
                        72,
                        504,
                        95
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                72,
                                504,
                                95
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        95
                                    ],
                                    "type": "text",
                                    "content": "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        100,
                        505,
                        124
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                100,
                                505,
                                124
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        100,
                                        505,
                                        124
                                    ],
                                    "type": "text",
                                    "content": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this."
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        183,
                        144,
                        411,
                        159
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                183,
                                144,
                                411,
                                159
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        183,
                                        144,
                                        411,
                                        159
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\operatorname {M u l t i H e a d} (Q, K, V) = \\operatorname {C o n c a t} \\left(\\operatorname {h e a d} _ {1},..., \\operatorname {h e a d} _ {\\mathrm {h}}\\right) W ^ {O}",
                                    "image_path": "b2b0fcbcb1590adf9b754c46e2847a95de1a6959934c08769a9dfec0d10fe8c2.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        222,
                        160,
                        425,
                        175
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                222,
                                160,
                                425,
                                175
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        222,
                                        160,
                                        425,
                                        175
                                    ],
                                    "type": "interline_equation",
                                    "content": "w h e r e \\quad \\text {h e a d} _ {\\mathrm {i}} = \\text {A t t e n t i o n} \\left(Q W _ {i} ^ {Q}, K W _ {i} ^ {K}, V W _ {i} ^ {V}\\right)",
                                    "image_path": "59552b139c619ad9d9c604f552cae64624e9f8bafe4b4b7283650c7007b1acc3.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        202,
                        504,
                        227
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                202,
                                504,
                                227
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "text",
                                    "content": "Where the projections are parameter matrices "
                                },
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "inline_equation",
                                    "content": "W_{i}^{Q}\\in \\mathbb{R}^{d_{\\mathrm{model}}\\times d_{k}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "text",
                                    "content": ", "
                                },
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "inline_equation",
                                    "content": "W_{i}^{K}\\in \\mathbb{R}^{d_{\\mathrm{model}}\\times d_{k}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "text",
                                    "content": ", "
                                },
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "inline_equation",
                                    "content": "W_{i}^{V}\\in \\mathbb{R}^{d_{\\mathrm{model}}\\times d_{v}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "text",
                                    "content": " and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "inline_equation",
                                    "content": "W^{O}\\in \\mathbb{R}^{hd_{v}\\times d_{\\mathrm{model}}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        202,
                                        504,
                                        227
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        104,
                        232,
                        505,
                        266
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                232,
                                505,
                                266
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        232,
                                        505,
                                        266
                                    ],
                                    "type": "text",
                                    "content": "In this work we employ "
                                },
                                {
                                    "bbox": [
                                        104,
                                        232,
                                        505,
                                        266
                                    ],
                                    "type": "inline_equation",
                                    "content": "h = 8"
                                },
                                {
                                    "bbox": [
                                        104,
                                        232,
                                        505,
                                        266
                                    ],
                                    "type": "text",
                                    "content": " parallel attention layers, or heads. For each of these we use "
                                },
                                {
                                    "bbox": [
                                        104,
                                        232,
                                        505,
                                        266
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{k} = d_{v} = d_{\\mathrm{model}} / h = 64"
                                },
                                {
                                    "bbox": [
                                        104,
                                        232,
                                        505,
                                        266
                                    ],
                                    "type": "text",
                                    "content": ". Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        278,
                        304,
                        290
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                278,
                                304,
                                290
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        278,
                                        304,
                                        290
                                    ],
                                    "type": "text",
                                    "content": "3.2.3 Applications of Attention in our Model"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        297,
                        373,
                        309
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                297,
                                373,
                                309
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        297,
                                        373,
                                        309
                                    ],
                                    "type": "text",
                                    "content": "The Transformer uses multi-head attention in three different ways:"
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        132,
                        319,
                        506,
                        482
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 11,
                    "blocks": [
                        {
                            "bbox": [
                                132,
                                319,
                                504,
                                373
                            ],
                            "type": "text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        132,
                                        319,
                                        504,
                                        373
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                132,
                                                319,
                                                504,
                                                373
                                            ],
                                            "type": "text",
                                            "content": "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                132,
                                378,
                                504,
                                422
                            ],
                            "type": "text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        132,
                                        378,
                                        504,
                                        422
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                132,
                                                378,
                                                504,
                                                422
                                            ],
                                            "type": "text",
                                            "content": "- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                132,
                                426,
                                506,
                                482
                            ],
                            "type": "text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        132,
                                        426,
                                        506,
                                        482
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                132,
                                                426,
                                                506,
                                                482
                                            ],
                                            "type": "text",
                                            "content": "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to "
                                        },
                                        {
                                            "bbox": [
                                                132,
                                                426,
                                                506,
                                                482
                                            ],
                                            "type": "inline_equation",
                                            "content": "-\\infty"
                                        },
                                        {
                                            "bbox": [
                                                132,
                                                426,
                                                506,
                                                482
                                            ],
                                            "type": "text",
                                            "content": ") all values in the input of the softmax which correspond to illegal connections. See Figure 2."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        }
                    ],
                    "sub_type": "text"
                },
                {
                    "bbox": [
                        105,
                        495,
                        293,
                        506
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                495,
                                293,
                                506
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        495,
                                        293,
                                        506
                                    ],
                                    "type": "text",
                                    "content": "3.3 Position-wise Feed-Forward Networks"
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        104,
                        516,
                        505,
                        549
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                516,
                                505,
                                549
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        516,
                                        505,
                                        549
                                    ],
                                    "type": "text",
                                    "content": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between."
                                }
                            ]
                        }
                    ],
                    "index": 13
                },
                {
                    "bbox": [
                        223,
                        566,
                        505,
                        578
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                223,
                                566,
                                505,
                                578
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        223,
                                        566,
                                        505,
                                        578
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\operatorname {F F N} (x) = \\max  \\left(0, x W _ {1} + b _ {1}\\right) W _ {2} + b _ {2} \\tag {2}",
                                    "image_path": "86d6b2671f1a6da0908e6afc0c1874d6aaa1a928f41467f1c3b682ab6ec6e4d2.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 14
                },
                {
                    "bbox": [
                        104,
                        588,
                        506,
                        634
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                588,
                                506,
                                634
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        588,
                                        506,
                                        634
                                    ],
                                    "type": "text",
                                    "content": "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is "
                                },
                                {
                                    "bbox": [
                                        104,
                                        588,
                                        506,
                                        634
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{\\mathrm{model}} = 512"
                                },
                                {
                                    "bbox": [
                                        104,
                                        588,
                                        506,
                                        634
                                    ],
                                    "type": "text",
                                    "content": ", and the inner-layer has dimensionality "
                                },
                                {
                                    "bbox": [
                                        104,
                                        588,
                                        506,
                                        634
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{ff} = 2048"
                                },
                                {
                                    "bbox": [
                                        104,
                                        588,
                                        506,
                                        634
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 15
                },
                {
                    "bbox": [
                        105,
                        646,
                        241,
                        658
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                646,
                                241,
                                658
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        646,
                                        241,
                                        658
                                    ],
                                    "type": "text",
                                    "content": "3.4 Embeddings and Softmax"
                                }
                            ]
                        }
                    ],
                    "index": 16
                },
                {
                    "bbox": [
                        104,
                        667,
                        506,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                667,
                                506,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension "
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{\\mathrm{model}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": ". We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by "
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\sqrt{d_{\\mathrm{model}}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        667,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 17
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "5"
                                }
                            ]
                        }
                    ],
                    "index": 18
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 4
        },
        {
            "para_blocks": [
                {
                    "type": "table",
                    "bbox": [
                        116,
                        110,
                        495,
                        188
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                116,
                                110,
                                495,
                                188
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        116,
                                        110,
                                        495,
                                        188
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                116,
                                                110,
                                                495,
                                                188
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Layer Type</td><td>Complexity per Layer</td><td>Sequential Operations</td><td>Maximum Path Length</td></tr><tr><td>Self-Attention</td><td>O(n2·d)</td><td>O(1)</td><td>O(1)</td></tr><tr><td>Recurrent</td><td>O(n·d2)</td><td>O(n)</td><td>O(n)</td></tr><tr><td>Convolutional</td><td>O(k·n·d2)</td><td>O(1)</td><td>O(logk(n))</td></tr><tr><td>Self-Attention (restricted)</td><td>O(r·n·d)</td><td>O(1)</td><td>O(n/r)</td></tr></table>",
                                            "image_path": "bd63cd8ef663420b999bad3a3aae91811bd7280633013dfcf7da5b6af166555e.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                104,
                                69,
                                504,
                                102
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        69,
                                        504,
                                        102
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "text",
                                            "content": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "inline_equation",
                                            "content": "n"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "text",
                                            "content": " is the sequence length, "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "inline_equation",
                                            "content": "d"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "text",
                                            "content": " is the representation dimension, "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "inline_equation",
                                            "content": "k"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "text",
                                            "content": " is the kernel size of convolutions and "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "inline_equation",
                                            "content": "r"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                102
                                            ],
                                            "type": "text",
                                            "content": " the size of the neighborhood in restricted self-attention."
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        105,
                        212,
                        216,
                        224
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                212,
                                216,
                                224
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        212,
                                        216,
                                        224
                                    ],
                                    "type": "text",
                                    "content": "3.5 Positional Encoding"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        232,
                        504,
                        299
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                232,
                                504,
                                299
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        232,
                                        504,
                                        299
                                    ],
                                    "type": "text",
                                    "content": "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension "
                                },
                                {
                                    "bbox": [
                                        104,
                                        232,
                                        504,
                                        299
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{\\mathrm{model}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        232,
                                        504,
                                        299
                                    ],
                                    "type": "text",
                                    "content": " as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        105,
                        303,
                        389,
                        316
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                303,
                                389,
                                316
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        303,
                                        389,
                                        316
                                    ],
                                    "type": "text",
                                    "content": "In this work, we use sine and cosine functions of different frequencies:"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        235,
                        335,
                        385,
                        349
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                235,
                                335,
                                385,
                                349
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        235,
                                        335,
                                        385,
                                        349
                                    ],
                                    "type": "interline_equation",
                                    "content": "P E _ {(p o s, 2 i)} = \\sin (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\mathrm {m o d e l}}})",
                                    "image_path": "15f147b860ad8b4693886a3fd8ba2cba1755f871cef6fe8e772281b5f0c180a4.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        225,
                        352,
                        385,
                        365
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                225,
                                352,
                                385,
                                365
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        225,
                                        352,
                                        385,
                                        365
                                    ],
                                    "type": "interline_equation",
                                    "content": "P E _ {(p o s, 2 i + 1)} = c o s (p o s / 1 0 0 0 0 ^ {2 i / d _ {\\mathrm {m o d e l}}})",
                                    "image_path": "24e3898f39f8c5cd97b756d2bf00fb84dca521a239567f0c01e34e017d2b6dd3.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        376,
                        504,
                        432
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                376,
                                504,
                                432
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "text",
                                    "content": "where "
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "inline_equation",
                                    "content": "pos"
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "text",
                                    "content": " is the position and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "inline_equation",
                                    "content": "i"
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "text",
                                    "content": " is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from "
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "inline_equation",
                                    "content": "2\\pi"
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "text",
                                    "content": " to "
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "inline_equation",
                                    "content": "10000 \\cdot 2\\pi"
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "text",
                                    "content": ". We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset "
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "inline_equation",
                                    "content": "k"
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "text",
                                    "content": ", "
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "inline_equation",
                                    "content": "PE_{pos+k}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "text",
                                    "content": " can be represented as a linear function of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "inline_equation",
                                    "content": "PE_{pos}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        376,
                                        504,
                                        432
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        435,
                        504,
                        480
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                435,
                                504,
                                480
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        435,
                                        504,
                                        480
                                    ],
                                    "type": "text",
                                    "content": "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training."
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        105,
                        495,
                        225,
                        509
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                495,
                                225,
                                509
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        495,
                                        225,
                                        509
                                    ],
                                    "type": "text",
                                    "content": "4 Why Self-Attention"
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        104,
                        519,
                        504,
                        574
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                519,
                                504,
                                574
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        519,
                                        504,
                                        574
                                    ],
                                    "type": "text",
                                    "content": "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations "
                                },
                                {
                                    "bbox": [
                                        104,
                                        519,
                                        504,
                                        574
                                    ],
                                    "type": "inline_equation",
                                    "content": "(x_{1},\\ldots ,x_{n})"
                                },
                                {
                                    "bbox": [
                                        104,
                                        519,
                                        504,
                                        574
                                    ],
                                    "type": "text",
                                    "content": " to another sequence of equal length "
                                },
                                {
                                    "bbox": [
                                        104,
                                        519,
                                        504,
                                        574
                                    ],
                                    "type": "inline_equation",
                                    "content": "(z_{1},\\ldots ,z_{n})"
                                },
                                {
                                    "bbox": [
                                        104,
                                        519,
                                        504,
                                        574
                                    ],
                                    "type": "text",
                                    "content": ", with "
                                },
                                {
                                    "bbox": [
                                        104,
                                        519,
                                        504,
                                        574
                                    ],
                                    "type": "inline_equation",
                                    "content": "x_{i},z_{i}\\in \\mathbb{R}^{d}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        519,
                                        504,
                                        574
                                    ],
                                    "type": "text",
                                    "content": ", such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata."
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        104,
                        579,
                        504,
                        603
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                579,
                                504,
                                603
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        579,
                                        504,
                                        603
                                    ],
                                    "type": "text",
                                    "content": "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required."
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "bbox": [
                        104,
                        607,
                        504,
                        685
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                607,
                                504,
                                685
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        607,
                                        504,
                                        685
                                    ],
                                    "type": "text",
                                    "content": "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types."
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        104,
                        689,
                        504,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                689,
                                504,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        689,
                                        504,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires "
                                },
                                {
                                    "bbox": [
                                        104,
                                        689,
                                        504,
                                        723
                                    ],
                                    "type": "inline_equation",
                                    "content": "O(n)"
                                },
                                {
                                    "bbox": [
                                        104,
                                        689,
                                        504,
                                        723
                                    ],
                                    "type": "text",
                                    "content": " sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence"
                                }
                            ]
                        }
                    ],
                    "index": 13
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        742,
                        308,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                742,
                                308,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        742,
                                        308,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "6"
                                }
                            ]
                        }
                    ],
                    "index": 14
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 5
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        104,
                        72,
                        504,
                        139
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                72,
                                504,
                                139
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "text",
                                    "content": "length "
                                },
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "inline_equation",
                                    "content": "n"
                                },
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "text",
                                    "content": " is smaller than the representation dimensionality "
                                },
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "inline_equation",
                                    "content": "d"
                                },
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "text",
                                    "content": ", which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size "
                                },
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "inline_equation",
                                    "content": "r"
                                },
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "text",
                                    "content": " in the input sequence centered around the respective output position. This would increase the maximum path length to "
                                },
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "inline_equation",
                                    "content": "O(n / r)"
                                },
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        139
                                    ],
                                    "type": "text",
                                    "content": ". We plan to investigate this approach further in future work."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        144,
                        506,
                        232
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                144,
                                506,
                                232
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "text",
                                    "content": "A single convolutional layer with kernel width "
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "inline_equation",
                                    "content": "k < n"
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "text",
                                    "content": " does not connect all pairs of input and output positions. Doing so requires a stack of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "inline_equation",
                                    "content": "O(n / k)"
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "text",
                                    "content": " convolutional layers in the case of contiguous kernels, or "
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "inline_equation",
                                    "content": "O(\\log_k(n))"
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "text",
                                    "content": " in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "inline_equation",
                                    "content": "k"
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "text",
                                    "content": ". Separable convolutions [6], however, decrease the complexity considerably, to "
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "inline_equation",
                                    "content": "O(k \\cdot n \\cdot d + n \\cdot d^2)"
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "text",
                                    "content": ". Even with "
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "inline_equation",
                                    "content": "k = n"
                                },
                                {
                                    "bbox": [
                                        104,
                                        144,
                                        506,
                                        232
                                    ],
                                    "type": "text",
                                    "content": ", however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model."
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        236,
                        504,
                        281
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                236,
                                504,
                                281
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        236,
                                        504,
                                        281
                                    ],
                                    "type": "text",
                                    "content": "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        105,
                        298,
                        171,
                        312
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                298,
                                171,
                                312
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        298,
                                        171,
                                        312
                                    ],
                                    "type": "text",
                                    "content": "5 Training"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        105,
                        324,
                        337,
                        335
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                324,
                                337,
                                335
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        324,
                                        337,
                                        335
                                    ],
                                    "type": "text",
                                    "content": "This section describes the training regime for our models."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        105,
                        350,
                        250,
                        363
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                350,
                                250,
                                363
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        350,
                                        250,
                                        363
                                    ],
                                    "type": "text",
                                    "content": "5.1 Training Data and Batching"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        372,
                        506,
                        449
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                372,
                                506,
                                449
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        372,
                                        506,
                                        449
                                    ],
                                    "type": "text",
                                    "content": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        105,
                        464,
                        234,
                        475
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                464,
                                234,
                                475
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        464,
                                        234,
                                        475
                                    ],
                                    "type": "text",
                                    "content": "5.2 Hardware and Schedule"
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        484,
                        504,
                        540
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                484,
                                504,
                                540
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        484,
                                        504,
                                        540
                                    ],
                                    "type": "text",
                                    "content": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days)."
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        105,
                        555,
                        175,
                        567
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                555,
                                175,
                                567
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        555,
                                        175,
                                        567
                                    ],
                                    "type": "text",
                                    "content": "5.3 Optimizer"
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        104,
                        576,
                        504,
                        599
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                576,
                                504,
                                599
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        576,
                                        504,
                                        599
                                    ],
                                    "type": "text",
                                    "content": "We used the Adam optimizer [20] with "
                                },
                                {
                                    "bbox": [
                                        104,
                                        576,
                                        504,
                                        599
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\beta_{1} = 0.9"
                                },
                                {
                                    "bbox": [
                                        104,
                                        576,
                                        504,
                                        599
                                    ],
                                    "type": "text",
                                    "content": ", "
                                },
                                {
                                    "bbox": [
                                        104,
                                        576,
                                        504,
                                        599
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\beta_{2} = 0.98"
                                },
                                {
                                    "bbox": [
                                        104,
                                        576,
                                        504,
                                        599
                                    ],
                                    "type": "text",
                                    "content": " and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        576,
                                        504,
                                        599
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\epsilon = 10^{-9}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        576,
                                        504,
                                        599
                                    ],
                                    "type": "text",
                                    "content": ". We varied the learning rate over the course of training, according to the formula:"
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        159,
                        616,
                        505,
                        631
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                159,
                                616,
                                505,
                                631
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        159,
                                        616,
                                        505,
                                        631
                                    ],
                                    "type": "interline_equation",
                                    "content": "l r a t e = d _ {\\text {m o d e l}} ^ {- 0. 5} \\cdot \\min  \\left(\\operatorname {s t e p} _ {-} \\operatorname {n u m} ^ {- 0. 5}, \\operatorname {s t e p} _ {-} \\operatorname {n u m} \\cdot \\operatorname {w a r m u p} _ {-} \\operatorname {s t e p s} ^ {- 1. 5}\\right) \\tag {3}",
                                    "image_path": "008d5ae889b32373bc314fabc09e38cc65cace6e71e55f870bcd4f762632a453.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "bbox": [
                        104,
                        641,
                        506,
                        675
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                641,
                                506,
                                675
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        641,
                                        506,
                                        675
                                    ],
                                    "type": "text",
                                    "content": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000."
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        105,
                        689,
                        194,
                        701
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                689,
                                194,
                                701
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        689,
                                        194,
                                        701
                                    ],
                                    "type": "text",
                                    "content": "5.4 Regularization"
                                }
                            ]
                        }
                    ],
                    "index": 13
                },
                {
                    "bbox": [
                        105,
                        711,
                        332,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                711,
                                332,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        711,
                                        332,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "We employ three types of regularization during training:"
                                }
                            ]
                        }
                    ],
                    "index": 14
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "7"
                                }
                            ]
                        }
                    ],
                    "index": 15
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 6
        },
        {
            "para_blocks": [
                {
                    "type": "table",
                    "bbox": [
                        127,
                        94,
                        482,
                        245
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                127,
                                94,
                                482,
                                245
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        127,
                                        94,
                                        482,
                                        245
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                127,
                                                94,
                                                482,
                                                245
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">BLEU</td><td colspan=\"2\">Training Cost (FLOPs)</td></tr><tr><td>EN-DE</td><td>EN-FR</td><td>EN-DE</td><td>EN-FR</td></tr><tr><td>ByteNet [18]</td><td>23.75</td><td></td><td></td><td></td></tr><tr><td>Deep-Att + PosUnk [39]</td><td></td><td>39.2</td><td></td><td>1.0 · 1020</td></tr><tr><td>GNMT + RL [38]</td><td>24.6</td><td>39.92</td><td>2.3 · 1019</td><td>1.4 · 1020</td></tr><tr><td>ConvS2S [9]</td><td>25.16</td><td>40.46</td><td>9.6 · 1018</td><td>1.5 · 1020</td></tr><tr><td>MoE [32]</td><td>26.03</td><td>40.56</td><td>2.0 · 1019</td><td>1.2 · 1020</td></tr><tr><td>Deep-Att + PosUnk Ensemble [39]</td><td></td><td>40.4</td><td></td><td>8.0 · 1020</td></tr><tr><td>GNMT + RL Ensemble [38]</td><td>26.30</td><td>41.16</td><td>1.8 · 1020</td><td>1.1 · 1021</td></tr><tr><td>ConvS2S Ensemble [9]</td><td>26.36</td><td>41.29</td><td>7.7 · 1019</td><td>1.2 · 1021</td></tr><tr><td>Transformer (base model)</td><td>27.3</td><td>38.1</td><td colspan=\"2\">3.3 · 1018</td></tr><tr><td>Transformer (big)</td><td>28.4</td><td>41.8</td><td colspan=\"2\">2.3 · 1019</td></tr></table>",
                                            "image_path": "d417a63d9d480919dfe427682f3cd8cd08a1cfce8623736fbf76a84de208b8b8.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                105,
                                69,
                                504,
                                92
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        105,
                                        69,
                                        504,
                                        92
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                105,
                                                69,
                                                504,
                                                92
                                            ],
                                            "type": "text",
                                            "content": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost."
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        271,
                        504,
                        317
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                271,
                                504,
                                317
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        271,
                                        504,
                                        317
                                    ],
                                    "type": "text",
                                    "content": "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        271,
                                        504,
                                        317
                                    ],
                                    "type": "inline_equation",
                                    "content": "P_{drop} = 0.1"
                                },
                                {
                                    "bbox": [
                                        104,
                                        271,
                                        504,
                                        317
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        329,
                        504,
                        353
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                329,
                                504,
                                353
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        329,
                                        504,
                                        353
                                    ],
                                    "type": "text",
                                    "content": "Label Smoothing During training, we employed label smoothing of value "
                                },
                                {
                                    "bbox": [
                                        104,
                                        329,
                                        504,
                                        353
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\epsilon_{ls} = 0.1"
                                },
                                {
                                    "bbox": [
                                        104,
                                        329,
                                        504,
                                        353
                                    ],
                                    "type": "text",
                                    "content": " [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        105,
                        369,
                        164,
                        382
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                369,
                                164,
                                382
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        369,
                                        164,
                                        382
                                    ],
                                    "type": "text",
                                    "content": "6 Results"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        105,
                        395,
                        219,
                        406
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                395,
                                219,
                                406
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        395,
                                        219,
                                        406
                                    ],
                                    "type": "text",
                                    "content": "6.1 Machine Translation"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        416,
                        504,
                        482
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                416,
                                504,
                                482
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        416,
                                        504,
                                        482
                                    ],
                                    "type": "text",
                                    "content": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        487,
                        504,
                        533
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                487,
                                504,
                                533
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        487,
                                        504,
                                        533
                                    ],
                                    "type": "text",
                                    "content": "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than "
                                },
                                {
                                    "bbox": [
                                        104,
                                        487,
                                        504,
                                        533
                                    ],
                                    "type": "inline_equation",
                                    "content": "1/4"
                                },
                                {
                                    "bbox": [
                                        104,
                                        487,
                                        504,
                                        533
                                    ],
                                    "type": "text",
                                    "content": " the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate "
                                },
                                {
                                    "bbox": [
                                        104,
                                        487,
                                        504,
                                        533
                                    ],
                                    "type": "inline_equation",
                                    "content": "P_{drop} = 0.1"
                                },
                                {
                                    "bbox": [
                                        104,
                                        487,
                                        504,
                                        533
                                    ],
                                    "type": "text",
                                    "content": ", instead of 0.3."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        536,
                        504,
                        592
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                536,
                                504,
                                592
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        536,
                                        504,
                                        592
                                    ],
                                    "type": "text",
                                    "content": "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty "
                                },
                                {
                                    "bbox": [
                                        104,
                                        536,
                                        504,
                                        592
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\alpha = 0.6"
                                },
                                {
                                    "bbox": [
                                        104,
                                        536,
                                        504,
                                        592
                                    ],
                                    "type": "text",
                                    "content": " [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length "
                                },
                                {
                                    "bbox": [
                                        104,
                                        536,
                                        504,
                                        592
                                    ],
                                    "type": "inline_equation",
                                    "content": "+50"
                                },
                                {
                                    "bbox": [
                                        104,
                                        536,
                                        504,
                                        592
                                    ],
                                    "type": "text",
                                    "content": ", but terminate early when possible [38]."
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        104,
                        596,
                        504,
                        641
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                596,
                                504,
                                641
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        596,
                                        504,
                                        641
                                    ],
                                    "type": "text",
                                    "content": "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU "
                                },
                                {
                                    "bbox": [
                                        104,
                                        596,
                                        504,
                                        641
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{5}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        596,
                                        504,
                                        641
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        105,
                        655,
                        204,
                        667
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                655,
                                204,
                                667
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        655,
                                        204,
                                        667
                                    ],
                                    "type": "text",
                                    "content": "6.2 Model Variations"
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        104,
                        677,
                        504,
                        700
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                677,
                                504,
                                700
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        677,
                                        504,
                                        700
                                    ],
                                    "type": "text",
                                    "content": "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the"
                                }
                            ]
                        }
                    ],
                    "index": 11
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        116,
                        710,
                        453,
                        722
                    ],
                    "type": "page_footnote",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                116,
                                710,
                                453,
                                722
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        116,
                                        710,
                                        453,
                                        722
                                    ],
                                    "type": "text",
                                    "content": "<sup>5</sup>We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively."
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        302,
                        741,
                        308,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                308,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        308,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "8"
                                }
                            ]
                        }
                    ],
                    "index": 13
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 7
        },
        {
            "para_blocks": [
                {
                    "type": "table",
                    "bbox": [
                        106,
                        128,
                        509,
                        384
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                128,
                                509,
                                384
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        128,
                                        509,
                                        384
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                128,
                                                509,
                                                384
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td></td><td>N</td><td>dmodel</td><td>df</td><td>h</td><td>dk</td><td>dv</td><td>Pdrop</td><td>εls</td><td>train steps</td><td>PPL (dev)</td><td>BLEU (dev)</td><td>params ×106</td></tr><tr><td>base</td><td>6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>64</td><td>0.1</td><td>0.1</td><td>100K</td><td>4.92</td><td>25.8</td><td>65</td></tr><tr><td rowspan=\"4\">(A)</td><td></td><td></td><td></td><td>1</td><td>512</td><td>512</td><td></td><td></td><td></td><td>5.29</td><td>24.9</td><td></td></tr><tr><td></td><td></td><td></td><td>4</td><td>128</td><td>128</td><td></td><td></td><td></td><td>5.00</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td>16</td><td>32</td><td>32</td><td></td><td></td><td></td><td>4.91</td><td>25.8</td><td></td></tr><tr><td></td><td></td><td></td><td>32</td><td>16</td><td>16</td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td></td></tr><tr><td rowspan=\"2\">(B)</td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td></td><td>5.16</td><td>25.1</td><td>58</td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td>60</td></tr><tr><td rowspan=\"7\">(C)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>23.7</td><td>36</td></tr><tr><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.19</td><td>25.3</td><td>50</td></tr><tr><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.88</td><td>25.5</td><td>80</td></tr><tr><td></td><td>256</td><td></td><td></td><td>32</td><td>32</td><td></td><td></td><td></td><td>5.75</td><td>24.5</td><td>28</td></tr><tr><td></td><td>1024</td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>168</td></tr><tr><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.12</td><td>25.4</td><td>53</td></tr><tr><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.75</td><td>26.2</td><td>90</td></tr><tr><td rowspan=\"4\">(D)</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>5.77</td><td>24.6</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>4.95</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>4.67</td><td>25.3</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>5.47</td><td>25.7</td><td></td></tr><tr><td>(E)</td><td colspan=\"9\">positional embedding instead of sinusoids</td><td>4.92</td><td>25.7</td><td></td></tr><tr><td>big</td><td>6</td><td>1024</td><td>4096</td><td>16</td><td></td><td></td><td>0.3</td><td></td><td>300K</td><td>4.33</td><td>26.4</td><td>213</td></tr></table>",
                                            "image_path": "e3fb9cad957f9b4bf3ca0f51f12310d12ce648d3b8229acf2b011bd17d6dce25.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                104,
                                69,
                                504,
                                114
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        69,
                                        504,
                                        114
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                114
                                            ],
                                            "type": "text",
                                            "content": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities."
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        411,
                        504,
                        435
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                411,
                                504,
                                435
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        411,
                                        504,
                                        435
                                    ],
                                    "type": "text",
                                    "content": "development set, newtest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        439,
                        504,
                        473
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                439,
                                504,
                                473
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        439,
                                        504,
                                        473
                                    ],
                                    "type": "text",
                                    "content": "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        477,
                        504,
                        544
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                477,
                                504,
                                544
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        477,
                                        504,
                                        544
                                    ],
                                    "type": "text",
                                    "content": "In Table 3 rows (B), we observe that reducing the attention key size "
                                },
                                {
                                    "bbox": [
                                        104,
                                        477,
                                        504,
                                        544
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{k}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        477,
                                        504,
                                        544
                                    ],
                                    "type": "text",
                                    "content": " hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        105,
                        559,
                        257,
                        571
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                559,
                                257,
                                571
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        559,
                                        257,
                                        571
                                    ],
                                    "type": "text",
                                    "content": "6.3 English Constituency Parsing"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        579,
                        504,
                        624
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                579,
                                504,
                                624
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        579,
                                        504,
                                        624
                                    ],
                                    "type": "text",
                                    "content": "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        628,
                        504,
                        685
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                628,
                                504,
                                685
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        685
                                    ],
                                    "type": "text",
                                    "content": "We trained a 4-layer transformer with "
                                },
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        685
                                    ],
                                    "type": "inline_equation",
                                    "content": "d_{model} = 1024"
                                },
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        685
                                    ],
                                    "type": "text",
                                    "content": " on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        689,
                        504,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                689,
                                504,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        689,
                                        504,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we"
                                }
                            ]
                        }
                    ],
                    "index": 8
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "9"
                                }
                            ]
                        }
                    ],
                    "index": 9
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 8
        },
        {
            "para_blocks": [
                {
                    "type": "table",
                    "bbox": [
                        143,
                        92,
                        468,
                        237
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                143,
                                92,
                                468,
                                237
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        143,
                                        92,
                                        468,
                                        237
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                143,
                                                92,
                                                468,
                                                237
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Parser</td><td>Training</td><td>WSJ 23 F1</td></tr><tr><td>Vinyals &amp; Kaiser el al. (2014) [37]</td><td>WSJ only, discriminative</td><td>88.3</td></tr><tr><td>Petrov et al. (2006) [29]</td><td>WSJ only, discriminative</td><td>90.4</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>WSJ only, discriminative</td><td>90.4</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>WSJ only, discriminative</td><td>91.7</td></tr><tr><td>Transformer (4 layers)</td><td>WSJ only, discriminative</td><td>91.3</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>Huang &amp; Harper (2009) [14]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>McClosky et al. (2006) [26]</td><td>semi-supervised</td><td>92.1</td></tr><tr><td>Vinyals &amp; Kaiser el al. (2014) [37]</td><td>semi-supervised</td><td>92.1</td></tr><tr><td>Transformer (4 layers)</td><td>semi-supervised</td><td>92.7</td></tr><tr><td>Luong et al. (2015) [23]</td><td>multi-task</td><td>93.0</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>generative</td><td>93.3</td></tr></table>",
                                            "image_path": "88cd2510dfa1298329d38bc4b12b4a9bf255189722089f1b862158963ec184a8.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                104,
                                69,
                                504,
                                90
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        69,
                                        504,
                                        90
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                69,
                                                504,
                                                90
                                            ],
                                            "type": "text",
                                            "content": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        260,
                        504,
                        283
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                260,
                                504,
                                283
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        260,
                                        504,
                                        283
                                    ],
                                    "type": "text",
                                    "content": "increased the maximum output length to input length "
                                },
                                {
                                    "bbox": [
                                        104,
                                        260,
                                        504,
                                        283
                                    ],
                                    "type": "inline_equation",
                                    "content": "+300"
                                },
                                {
                                    "bbox": [
                                        104,
                                        260,
                                        504,
                                        283
                                    ],
                                    "type": "text",
                                    "content": ". We used a beam size of 21 and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        260,
                                        504,
                                        283
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\alpha = 0.3"
                                },
                                {
                                    "bbox": [
                                        104,
                                        260,
                                        504,
                                        283
                                    ],
                                    "type": "text",
                                    "content": " for both WSJ only and the semi-supervised setting."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        287,
                        506,
                        321
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                287,
                                506,
                                321
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        287,
                                        506,
                                        321
                                    ],
                                    "type": "text",
                                    "content": "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        325,
                        506,
                        349
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                325,
                                506,
                                349
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        325,
                                        506,
                                        349
                                    ],
                                    "type": "text",
                                    "content": "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        105,
                        363,
                        185,
                        376
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                363,
                                185,
                                376
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        363,
                                        185,
                                        376
                                    ],
                                    "type": "text",
                                    "content": "7 Conclusion"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        387,
                        504,
                        420
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                387,
                                504,
                                420
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        387,
                                        504,
                                        420
                                    ],
                                    "type": "text",
                                    "content": "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        426,
                        505,
                        471
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                426,
                                505,
                                471
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        426,
                                        505,
                                        471
                                    ],
                                    "type": "text",
                                    "content": "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        474,
                        505,
                        520
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                474,
                                505,
                                520
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        474,
                                        505,
                                        520
                                    ],
                                    "type": "text",
                                    "content": "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours."
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        104,
                        523,
                        506,
                        546
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                523,
                                506,
                                546
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        523,
                                        506,
                                        546
                                    ],
                                    "type": "text",
                                    "content": "The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor."
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        104,
                        557,
                        504,
                        580
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                557,
                                504,
                                580
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        557,
                                        504,
                                        580
                                    ],
                                    "type": "text",
                                    "content": "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration."
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        105,
                        594,
                        165,
                        607
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                594,
                                165,
                                607
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        594,
                                        165,
                                        607
                                    ],
                                    "type": "text",
                                    "content": "References"
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "bbox": [
                        110,
                        613,
                        505,
                        723
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 16,
                    "blocks": [
                        {
                            "bbox": [
                                110,
                                613,
                                504,
                                635
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        110,
                                        613,
                                        504,
                                        635
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                110,
                                                613,
                                                504,
                                                635
                                            ],
                                            "type": "text",
                                            "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                110,
                                642,
                                505,
                                665
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        110,
                                        642,
                                        505,
                                        665
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                110,
                                                642,
                                                505,
                                                665
                                            ],
                                            "type": "text",
                                            "content": "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                110,
                                671,
                                504,
                                693
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        110,
                                        671,
                                        504,
                                        693
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                110,
                                                671,
                                                504,
                                                693
                                            ],
                                            "type": "text",
                                            "content": "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                110,
                                700,
                                504,
                                723
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        110,
                                        700,
                                        504,
                                        723
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                110,
                                                700,
                                                504,
                                                723
                                            ],
                                            "type": "text",
                                            "content": "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        312,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                312,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        312,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "10"
                                }
                            ]
                        }
                    ],
                    "index": 17
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 9
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        72,
                        506,
                        722
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 20,
                    "blocks": [
                        {
                            "bbox": [
                                112,
                                72,
                                505,
                                106
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        112,
                                        72,
                                        505,
                                        106
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                112,
                                                72,
                                                505,
                                                106
                                            ],
                                            "type": "text",
                                            "content": "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnnc encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                112,
                                114,
                                504,
                                137
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        112,
                                        114,
                                        504,
                                        137
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                112,
                                                114,
                                                504,
                                                137
                                            ],
                                            "type": "text",
                                            "content": "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                111,
                                144,
                                504,
                                168
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        144,
                                        504,
                                        168
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                144,
                                                504,
                                                168
                                            ],
                                            "type": "text",
                                            "content": "[7] Junyoung Chung, Caglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                111,
                                175,
                                504,
                                198
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        175,
                                        504,
                                        198
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                175,
                                                504,
                                                198
                                            ],
                                            "type": "text",
                                            "content": "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                111,
                                205,
                                506,
                                229
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        205,
                                        506,
                                        229
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                205,
                                                506,
                                                229
                                            ],
                                            "type": "text",
                                            "content": "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                107,
                                237,
                                504,
                                259
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        237,
                                        504,
                                        259
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                237,
                                                504,
                                                259
                                            ],
                                            "type": "text",
                                            "content": "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                106,
                                267,
                                506,
                                300
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        267,
                                        506,
                                        300
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                267,
                                                506,
                                                300
                                            ],
                                            "type": "text",
                                            "content": "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                107,
                                308,
                                504,
                                332
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        308,
                                        504,
                                        332
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                308,
                                                504,
                                                332
                                            ],
                                            "type": "text",
                                            "content": "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                107,
                                339,
                                506,
                                361
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        339,
                                        506,
                                        361
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                339,
                                                506,
                                                361
                                            ],
                                            "type": "text",
                                            "content": "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                107,
                                370,
                                504,
                                405
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        370,
                                        504,
                                        405
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                370,
                                                504,
                                                405
                                            ],
                                            "type": "text",
                                            "content": "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832-841. ACL, August 2009."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                107,
                                412,
                                504,
                                435
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        412,
                                        504,
                                        435
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                412,
                                                504,
                                                435
                                            ],
                                            "type": "text",
                                            "content": "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                107,
                                442,
                                504,
                                466
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        442,
                                        504,
                                        466
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                442,
                                                504,
                                                466
                                            ],
                                            "type": "text",
                                            "content": "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                107,
                                473,
                                504,
                                496
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        473,
                                        504,
                                        496
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                473,
                                                504,
                                                496
                                            ],
                                            "type": "text",
                                            "content": "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                107,
                                504,
                                506,
                                536
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        504,
                                        506,
                                        536
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                504,
                                                506,
                                                536
                                            ],
                                            "type": "text",
                                            "content": "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                107,
                                545,
                                506,
                                568
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        545,
                                        506,
                                        568
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                545,
                                                506,
                                                568
                                            ],
                                            "type": "text",
                                            "content": "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                107,
                                576,
                                506,
                                588
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        576,
                                        506,
                                        588
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                576,
                                                506,
                                                588
                                            ],
                                            "type": "text",
                                            "content": "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                107,
                                596,
                                504,
                                619
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        596,
                                        504,
                                        619
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                596,
                                                504,
                                                619
                                            ],
                                            "type": "text",
                                            "content": "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                107,
                                627,
                                504,
                                660
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        627,
                                        504,
                                        660
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                627,
                                                504,
                                                660
                                            ],
                                            "type": "text",
                                            "content": "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                107,
                                668,
                                504,
                                692
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        668,
                                        504,
                                        692
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                668,
                                                504,
                                                692
                                            ],
                                            "type": "text",
                                            "content": "[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                107,
                                700,
                                504,
                                722
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        700,
                                        504,
                                        722
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                700,
                                                504,
                                                722
                                            ],
                                            "type": "text",
                                            "content": "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        310,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                310,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        310,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "11"
                                }
                            ]
                        }
                    ],
                    "index": 21
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 10
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        72,
                        506,
                        720
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 16,
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                72,
                                505,
                                96
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        72,
                                        505,
                                        96
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                72,
                                                505,
                                                96
                                            ],
                                            "type": "text",
                                            "content": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                106,
                                106,
                                506,
                                140
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        106,
                                        506,
                                        140
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                106,
                                                506,
                                                140
                                            ],
                                            "type": "text",
                                            "content": "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152-159. ACL, June 2006."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                107,
                                152,
                                504,
                                175
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        152,
                                        504,
                                        175
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                152,
                                                504,
                                                175
                                            ],
                                            "type": "text",
                                            "content": "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                107,
                                186,
                                504,
                                210
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        186,
                                        504,
                                        210
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                186,
                                                504,
                                                210
                                            ],
                                            "type": "text",
                                            "content": "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                106,
                                220,
                                506,
                                265
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        220,
                                        506,
                                        265
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                220,
                                                506,
                                                265
                                            ],
                                            "type": "text",
                                            "content": "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433-440. ACL, July 2006."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                107,
                                276,
                                504,
                                300
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        276,
                                        504,
                                        300
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                276,
                                                504,
                                                300
                                            ],
                                            "type": "text",
                                            "content": "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                107,
                                311,
                                504,
                                335
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        311,
                                        504,
                                        335
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                311,
                                                504,
                                                335
                                            ],
                                            "type": "text",
                                            "content": "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                107,
                                345,
                                506,
                                380
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        345,
                                        506,
                                        380
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                345,
                                                506,
                                                380
                                            ],
                                            "type": "text",
                                            "content": "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                107,
                                390,
                                506,
                                425
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        390,
                                        506,
                                        425
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                390,
                                                506,
                                                425
                                            ],
                                            "type": "text",
                                            "content": "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929-1958, 2014."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                107,
                                436,
                                506,
                                480
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        436,
                                        506,
                                        480
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                436,
                                                506,
                                                480
                                            ],
                                            "type": "text",
                                            "content": "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440-2448. Curran Associates, Inc., 2015."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                107,
                                492,
                                504,
                                517
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        492,
                                        504,
                                        517
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                492,
                                                504,
                                                517
                                            ],
                                            "type": "text",
                                            "content": "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104-3112, 2014."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                107,
                                526,
                                506,
                                551
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        526,
                                        506,
                                        551
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                526,
                                                506,
                                                551
                                            ],
                                            "type": "text",
                                            "content": "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                107,
                                561,
                                504,
                                585
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        561,
                                        504,
                                        585
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                561,
                                                504,
                                                585
                                            ],
                                            "type": "text",
                                            "content": "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                107,
                                595,
                                504,
                                640
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        595,
                                        504,
                                        640
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                595,
                                                504,
                                                640
                                            ],
                                            "type": "text",
                                            "content": "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                107,
                                651,
                                504,
                                675
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        651,
                                        504,
                                        675
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                651,
                                                504,
                                                675
                                            ],
                                            "type": "text",
                                            "content": "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                107,
                                685,
                                504,
                                720
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        685,
                                        504,
                                        720
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                685,
                                                504,
                                                720
                                            ],
                                            "type": "text",
                                            "content": "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434-443. ACL, August 2013."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        311,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                311,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        311,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "12"
                                }
                            ]
                        }
                    ],
                    "index": 17
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 11
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        71,
                        231,
                        84
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                106,
                                71,
                                231,
                                84
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        106,
                                        71,
                                        231,
                                        84
                                    ],
                                    "type": "text",
                                    "content": "Attention Visualizations"
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "image",
                    "bbox": [
                        118,
                        98,
                        504,
                        301
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                118,
                                98,
                                504,
                                301
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        118,
                                        98,
                                        504,
                                        301
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                118,
                                                98,
                                                504,
                                                301
                                            ],
                                            "type": "image",
                                            "image_path": "9be8d51258cd6fe2526d8800d553f7108651a1d4c49433a59883f07396f92286.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                105,
                                310,
                                504,
                                355
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        105,
                                        310,
                                        504,
                                        355
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                105,
                                                310,
                                                504,
                                                355
                                            ],
                                            "type": "text",
                                            "content": "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color."
                                        }
                                    ]
                                }
                            ],
                            "index": 2,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 1
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        310,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                310,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        310,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "13"
                                }
                            ]
                        }
                    ],
                    "index": 3
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 12
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        120,
                        167,
                        504,
                        605
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                120,
                                167,
                                504,
                                605
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        120,
                                        167,
                                        504,
                                        605
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                120,
                                                167,
                                                504,
                                                605
                                            ],
                                            "type": "image",
                                            "image_path": "5f0519f587dfa1718c4bb9aeb8f44e1307c30bd07d803bf6ef7aee5b5b386a5a.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 270,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                105,
                                612,
                                504,
                                647
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        105,
                                        612,
                                        504,
                                        647
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                105,
                                                612,
                                                504,
                                                647
                                            ],
                                            "type": "text",
                                            "content": "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 0
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        311,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                311,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        311,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "14"
                                }
                            ]
                        }
                    ],
                    "index": 2
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 13
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        120,
                        182,
                        499,
                        372
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                120,
                                182,
                                499,
                                372
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        120,
                                        182,
                                        499,
                                        372
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                120,
                                                182,
                                                499,
                                                372
                                            ],
                                            "type": "image",
                                            "image_path": "dd29c4dbc9b873b082c9f751cc974e6254bc431b82d886c0dde12e3cf70f1901.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_body"
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "image",
                    "bbox": [
                        119,
                        401,
                        499,
                        591
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                119,
                                401,
                                499,
                                591
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        119,
                                        401,
                                        499,
                                        591
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                119,
                                                401,
                                                499,
                                                591
                                            ],
                                            "type": "image",
                                            "image_path": "b10b80c3e7207426e12d484bdd3fbbe0922c099c00d0f04538d7f903e536684e.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                106,
                                601,
                                504,
                                634
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        601,
                                        504,
                                        634
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                601,
                                                504,
                                                634
                                            ],
                                            "type": "text",
                                            "content": "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks."
                                        }
                                    ]
                                }
                            ],
                            "index": 2,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 1
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        301,
                        741,
                        310,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                301,
                                741,
                                310,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        301,
                                        741,
                                        310,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "15"
                                }
                            ]
                        }
                    ],
                    "index": 3
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 14
        }
    ],
    "_backend": "vlm",
    "_version_name": "2.5.3"
}